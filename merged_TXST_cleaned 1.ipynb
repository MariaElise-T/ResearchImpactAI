{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23b4c341-2ed6-424c-a082-86f56a2eefaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysx4\\AppData\\Local\\Temp\\ipykernel_22252\\3381413154.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  resolved_duplicates = potential_duplicates.groupby(group_columns).apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'merged_TXST_uncleaned.csv'  # Update with the correct path\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Step 1: Identify and remove exact duplicates\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Step 2: Identify potential duplicates with minor differences\n",
    "group_columns = ['Title', 'Authors', 'Year']  # Example columns; adjust as necessary\n",
    "potential_duplicates = df_cleaned[df_cleaned.duplicated(subset=group_columns, keep=False)]\n",
    "\n",
    "# Step 3: Function to resolve duplicates by taking means or choosing appropriate values\n",
    "def resolve_duplicates(group):\n",
    "    # Prioritize journal over preprint if both are present\n",
    "    if 'PublicationType' in group.columns:\n",
    "        if 'Journal' in group['PublicationType'].values and 'Preprint' in group['PublicationType'].values:\n",
    "            group = group[group['PublicationType'] != 'Preprint']\n",
    "    \n",
    "    # Handle numeric columns by taking the mean\n",
    "    numeric_cols = group.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        group[col] = group[col].mean()\n",
    "    \n",
    "    # Handle categorical columns by taking the most common value (mode)\n",
    "    categorical_cols = group.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if col not in group_columns:  # Avoid altering the grouping columns\n",
    "            mode_value = group[col].mode()\n",
    "            if not mode_value.empty:\n",
    "                group[col] = mode_value[0]\n",
    "    \n",
    "    # If after resolving we still have more than one row, take the first row\n",
    "    if len(group) > 1:\n",
    "        return group.iloc[0]\n",
    "    else:\n",
    "        return group\n",
    "\n",
    "# Apply the resolve_duplicates function to each group of potential duplicates\n",
    "resolved_duplicates = potential_duplicates.groupby(group_columns).apply(\n",
    "    resolve_duplicates).reset_index(drop=True)\n",
    "\n",
    "# Step 4: Remove the original duplicates and replace them with the resolved ones\n",
    "df_cleaned = pd.concat([df_cleaned.drop(potential_duplicates.index), resolved_duplicates], ignore_index=True)\n",
    "\n",
    "# Step 5: Determine the total number of rows\n",
    "total_rows = df_cleaned.shape[0]\n",
    "\n",
    "# Step 6: Drop columns with 99% or more null, zero, or no values\n",
    "threshold = 0.99 * total_rows\n",
    "columns_to_drop = []\n",
    "\n",
    "for col in df_cleaned.columns:\n",
    "    null_count = df_cleaned[col].isnull().sum()\n",
    "    zero_count = (df_cleaned[col] == 0).sum()\n",
    "    no_value_count = null_count + zero_count\n",
    "    \n",
    "    if no_value_count >= threshold:\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "df_cleaned_reduced = df_cleaned.drop(columns=columns_to_drop)\n",
    "\n",
    "# Step 7: Drop more columns based on the 95% null/zero rule and importance\n",
    "threshold_95 = 0.95 * total_rows\n",
    "columns_to_consider_dropping = []\n",
    "\n",
    "for col in df_cleaned_reduced.columns:\n",
    "    null_count = df_cleaned_reduced[col].isnull().sum()\n",
    "    zero_count = (df_cleaned_reduced[col] == 0).sum()\n",
    "    no_value_count = null_count + zero_count\n",
    "    \n",
    "    if no_value_count >= threshold_95:\n",
    "        columns_to_consider_dropping.append(col)\n",
    "\n",
    "important_columns = [\n",
    "    'Authors', 'Title', 'Year', 'Source', 'Publisher', 'DOI', 'GSRank',\n",
    "    'Cites', 'CitesPerYear', 'CitesPerAuthor', 'AuthorCount', 'Journal', 'URL'\n",
    "]\n",
    "\n",
    "columns_to_drop_final = [col for col in columns_to_consider_dropping if col not in important_columns]\n",
    "df_final = df_cleaned_reduced.drop(columns=columns_to_drop_final)\n",
    "\n",
    "# Step 8: Replace null values with zeros where appropriate\n",
    "numeric_columns = df_final.select_dtypes(include=[np.number]).columns\n",
    "df_final[numeric_columns] = df_final[numeric_columns].fillna(0)\n",
    "\n",
    "# Step 9: Find and drop rows with 'query' in the Title column\n",
    "query_rows = df_final[df_final['Title'].str.contains('query', case=False, na=False)]\n",
    "df_final_no_query = df_final.drop(query_rows.index)\n",
    "\n",
    "# Save the final cleaned dataset if needed\n",
    "df_final_no_query.to_csv('cleaned_merged_dataset.csv', index=False)  # Update with the correct path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d99f58a-4c29-4836-81e6-0b67334f1087",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ysx4\\AppData\\Local\\Temp\\ipykernel_14560\\2457925163.py:42: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  resolved_duplicates = potential_duplicates.groupby(group_columns).apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'merged_TXST_uncleaned.csv'  # Update with the correct path\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "\n",
    "# Step 1: Identify and remove exact duplicates\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# Step 2: Identify potential duplicates with minor differences\n",
    "group_columns = ['Title', 'Authors', 'Year']  # Example columns; adjust as necessary\n",
    "potential_duplicates = df_cleaned[df_cleaned.duplicated(subset=group_columns, keep=False)]\n",
    "\n",
    "# Step 3: Function to resolve duplicates by taking means or choosing appropriate values\n",
    "def resolve_duplicates(group):\n",
    "    # Prioritize journal over preprint if both are present\n",
    "    if 'PublicationType' in group.columns:\n",
    "        if 'Journal' in group['PublicationType'].values and 'Preprint' in group['PublicationType'].values:\n",
    "            group = group[group['PublicationType'] != 'Preprint']\n",
    "    \n",
    "    # Handle numeric columns by taking the mean\n",
    "    numeric_cols = group.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        group[col] = group[col].mean()\n",
    "    \n",
    "    # Handle categorical columns by taking the most common value (mode)\n",
    "    categorical_cols = group.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if col not in group_columns:  # Avoid altering the grouping columns\n",
    "            mode_value = group[col].mode()\n",
    "            if not mode_value.empty:\n",
    "                group[col] = mode_value[0]\n",
    "    \n",
    "    # If after resolving we still have more than one row, take the first row\n",
    "    if len(group) > 1:\n",
    "        return group.iloc[0]\n",
    "    else:\n",
    "        return group\n",
    "\n",
    "# Apply the resolve_duplicates function to each group of potential duplicates\n",
    "resolved_duplicates = potential_duplicates.groupby(group_columns).apply(\n",
    "    resolve_duplicates).reset_index(drop=True)\n",
    "\n",
    "# Step 4: Remove the original duplicates and replace them with the resolved ones\n",
    "df_cleaned = pd.concat([df_cleaned.drop(potential_duplicates.index), resolved_duplicates], ignore_index=True)\n",
    "\n",
    "# Step 5: Determine the total number of rows\n",
    "total_rows = df_cleaned.shape[0]\n",
    "\n",
    "# Step 6: Drop columns with more than 97.5% null values, but retain specific important columns\n",
    "threshold = 0.975 * total_rows\n",
    "columns_to_drop = []\n",
    "\n",
    "# Important columns to retain, regardless of null percentage\n",
    "important_columns = ['msm_counts', 'wikipedia_counts']\n",
    "\n",
    "for col in df_cleaned.columns:\n",
    "    null_count = df_cleaned[col].isnull().sum()\n",
    "    \n",
    "    if null_count > threshold and col not in important_columns:\n",
    "        columns_to_drop.append(col)\n",
    "\n",
    "df_cleaned_reduced = df_cleaned.drop(columns=columns_to_drop)\n",
    "\n",
    "# Step 7: Replace null values with zeros where appropriate\n",
    "numeric_columns = df_cleaned_reduced.select_dtypes(include=[np.number]).columns\n",
    "df_cleaned_reduced[numeric_columns] = df_cleaned_reduced[numeric_columns].fillna(0)\n",
    "\n",
    "# Step 8: Find and drop rows with 'query' in the Title column\n",
    "query_rows = df_cleaned_reduced[df_cleaned_reduced['Title'].str.contains('query', case=False, na=False)]\n",
    "df_final = df_cleaned_reduced.drop(query_rows.index)\n",
    "\n",
    "# Save the final cleaned dataset\n",
    "df_final.to_csv('cleaned_merged_dataset.csv', index=False)  # Update with the correct path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f2413a-eb24-469a-814f-30b6cb9248ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
